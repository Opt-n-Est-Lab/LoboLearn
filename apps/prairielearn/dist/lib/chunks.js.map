{"version":3,"file":"chunks.js","sourceRoot":"","sources":["../../src/lib/chunks.ts"],"names":[],"mappings":"AAAA,OAAO,KAAK,aAAa,MAAM,eAAe,CAAC;AAC/C,OAAO,KAAK,IAAI,MAAM,MAAM,CAAC;AAC7B,OAAO,EAAE,WAAW,IAAI,iBAAiB,EAAE,MAAM,QAAQ,CAAC;AAC1D,OAAO,KAAK,IAAI,MAAM,MAAM,CAAC;AAE7B,OAAO,EAAE,EAAE,EAAE,MAAM,oBAAoB,CAAC;AACxC,OAAO,EAAE,MAAM,EAAE,MAAM,sBAAsB,CAAC;AAC9C,OAAO,KAAK,KAAK,MAAM,OAAO,CAAC;AAC/B,OAAO,EAAE,MAAM,UAAU,CAAC;AAC1B,OAAO,KAAK,GAAG,MAAM,KAAK,CAAC;AAC3B,OAAO,EAAE,EAAE,IAAI,MAAM,EAAE,MAAM,MAAM,CAAC;AAEpC,OAAO,KAAK,UAAU,MAAM,2BAA2B,CAAC;AACxD,OAAO,EAAE,QAAQ,EAAE,MAAM,0BAA0B,CAAC;AACpD,OAAO,KAAK,KAAK,MAAM,wBAAwB,CAAC;AAEhD,OAAO,EAAE,wBAAwB,EAAE,MAAM,qBAAqB,CAAC;AAC/D,OAAO,KAAK,QAAQ,MAAM,sBAAsB,CAAC;AACjD,OAAO,EAAmB,MAAM,sBAAsB,CAAC;AAEvD,OAAO,EAAE,cAAc,EAAE,kBAAkB,EAAE,MAAM,UAAU,CAAC;AAC9D,OAAO,EAAE,KAAK,EAAE,MAAM,YAAY,CAAC;AACnC,OAAO,EAAE,MAAM,EAAE,MAAM,aAAa,CAAC;AACrC,OAAO,EAAkB,eAAe,EAAE,MAAM,kBAAkB,CAAC;AAEnE,MAAM,GAAG,GAAG,KAAK,CAAC,YAAY,CAAC,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;AA+IhD;;;GAGG;AACH,MAAM,UAAU,8BAA8B,CAAC,KAAoB;IACjE,QAAQ,KAAK,CAAC,IAAI,EAAE,CAAC;QACnB,KAAK,UAAU,CAAC;QAChB,KAAK,mBAAmB,CAAC;QACzB,KAAK,mBAAmB,CAAC;QACzB,KAAK,mBAAmB;YACtB,OAAO;gBACL,IAAI,EAAE,KAAK,CAAC,IAAI;aACjB,CAAC;QACJ,KAAK,2BAA2B;YAC9B,IAAI,CAAC,KAAK,CAAC,oBAAoB,EAAE,CAAC;gBAChC,MAAM,IAAI,KAAK,CAAC,6CAA6C,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YAC7E,CAAC;YACD,OAAO;gBACL,IAAI,EAAE,KAAK,CAAC,IAAI;gBAChB,kBAAkB,EAAE,KAAK,CAAC,oBAAoB;aAC/C,CAAC;QACJ,KAAK,uBAAuB;YAC1B,IAAI,CAAC,KAAK,CAAC,oBAAoB,EAAE,CAAC;gBAChC,MAAM,IAAI,KAAK,CAAC,6CAA6C,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YAC7E,CAAC;YACD,IAAI,CAAC,KAAK,CAAC,eAAe,EAAE,CAAC;gBAC3B,MAAM,IAAI,KAAK,CAAC,wCAAwC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YACxE,CAAC;YACD,OAAO;gBACL,IAAI,EAAE,KAAK,CAAC,IAAI;gBAChB,kBAAkB,EAAE,KAAK,CAAC,oBAAoB;gBAC9C,cAAc,EAAE,KAAK,CAAC,eAAe;aACtC,CAAC;QACJ,KAAK,UAAU;YACb,IAAI,CAAC,KAAK,CAAC,aAAa,EAAE,CAAC;gBACzB,MAAM,IAAI,KAAK,CAAC,sCAAsC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YACtE,CAAC;YACD,OAAO;gBACL,IAAI,EAAE,KAAK,CAAC,IAAI;gBAChB,YAAY,EAAE,KAAK,CAAC,aAAa;aAClC,CAAC;IACN,CAAC;AACH,CAAC;AAED;;GAEG;AACH,MAAM,UAAU,YAAY,CAAC,aAA4B;IACvD,QAAQ,aAAa,CAAC,IAAI,EAAE,CAAC;QAC3B,KAAK,UAAU,CAAC;QAChB,KAAK,mBAAmB,CAAC;QACzB,KAAK,mBAAmB,CAAC;QACzB,KAAK,mBAAmB;YACtB,OAAO,aAAa,CAAC,IAAI,CAAC;QAC5B,KAAK,UAAU;YACb,OAAO,IAAI,CAAC,IAAI,CAAC,WAAW,EAAE,aAAa,CAAC,YAAY,CAAC,CAAC;QAC5D,KAAK,2BAA2B;YAC9B,OAAO,IAAI,CAAC,IAAI,CACd,iBAAiB,EACjB,aAAa,CAAC,kBAAkB,EAChC,2BAA2B,CAC5B,CAAC;QACJ,KAAK,uBAAuB;YAC1B,OAAO,IAAI,CAAC,IAAI,CACd,iBAAiB,EACjB,aAAa,CAAC,kBAAkB,EAChC,aAAa,EACb,aAAa,CAAC,cAAc,EAC5B,uBAAuB,CACxB,CAAC;IACN,CAAC;AACH,CAAC;AAED;;;GAGG;AACH,MAAM,UAAU,kBAAkB,CAAC,UAAkB,EAAE,aAA4B;IACjF,OAAO,IAAI,CAAC,IAAI,CAAC,UAAU,EAAE,YAAY,CAAC,aAAa,CAAC,CAAC,CAAC;AAC5D,CAAC;AAED;;;;;;;GAOG;AACH,MAAM,CAAC,KAAK,UAAU,oBAAoB,CACxC,UAAkB,EAClB,OAAe,EACf,OAAe;IAEf,8EAA8E;IAC9E,4EAA4E;IAC5E,2EAA2E;IAC3E,8EAA8E;IAC9E,EAAE;IACF,2EAA2E;IAC3E,0EAA0E;IAC1E,8EAA8E;IAC9E,+EAA+E;IAC/E,MAAM,EAAE,MAAM,EAAE,cAAc,EAAE,GAAG,MAAM,IAAI,CAAC,SAAS,CAAC,aAAa,CAAC,IAAI,CAAC,CACzE,+BAA+B,EAC/B,EAAE,GAAG,EAAE,UAAU,EAAE,CACpB,CAAC;IACF,MAAM,QAAQ,GAAG,cAAc,CAAC,IAAI,EAAE,CAAC;IAEvC,MAAM,EAAE,MAAM,EAAE,UAAU,EAAE,GAAG,MAAM,IAAI,CAAC,SAAS,CAAC,aAAa,CAAC,IAAI,CAAC,CACrE,wBAAwB,OAAO,KAAK,OAAO,EAAE,EAC7C;QACE,GAAG,EAAE,UAAU;QACf,2EAA2E;QAC3E,2EAA2E;QAC3E,yEAAyE;QACzE,mEAAmE;QACnE,YAAY;QACZ,SAAS,EAAE,EAAE,GAAG,IAAI,GAAG,IAAI;KAC5B,CACF,CAAC;IACF,MAAM,YAAY,GAAG,UAAU,CAAC,IAAI,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;IAEnD,gDAAgD;IAChD,MAAM,oBAAoB,GAAG,YAAY,CAAC,GAAG,CAAC,CAAC,WAAW,EAAE,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,QAAQ,EAAE,WAAW,CAAC,CAAC,CAAC;IAEjG,iEAAiE;IACjE,MAAM,kBAAkB,GAAG,oBAAoB,CAAC,MAAM,CAAC,CAAC,mBAAmB,EAAE,EAAE,CAC7E,QAAQ,CAAC,UAAU,EAAE,mBAAmB,CAAC,CAC1C,CAAC;IAEF,uDAAuD;IACvD,OAAO,kBAAkB,CAAC,GAAG,CAAC,CAAC,mBAAmB,EAAE,EAAE,CACpD,IAAI,CAAC,QAAQ,CAAC,UAAU,EAAE,mBAAmB,CAAC,CAC/C,CAAC;AACJ,CAAC;AAED;;;;;;;GAOG;AACH,MAAM,UAAU,8BAA8B,CAC5C,YAAsB,EACtB,UAAsB;IAEtB,MAAM,YAAY,GAAiB;QACjC,QAAQ,EAAE,KAAK;QACf,iBAAiB,EAAE,KAAK;QACxB,iBAAiB,EAAE,KAAK;QACxB,iBAAiB,EAAE,KAAK;QACxB,eAAe,EAAE,EAAE;QACnB,SAAS,EAAE,IAAI,GAAG,EAAE;KACrB,CAAC;IAEF,YAAY,CAAC,OAAO,CAAC,CAAC,WAAW,EAAE,EAAE;QACnC,IAAI,WAAW,CAAC,UAAU,CAAC,WAAW,CAAC,EAAE,CAAC;YACxC,YAAY,CAAC,QAAQ,GAAG,IAAI,CAAC;QAC/B,CAAC;QACD,IAAI,WAAW,CAAC,UAAU,CAAC,oBAAoB,CAAC,EAAE,CAAC;YACjD,YAAY,CAAC,iBAAiB,GAAG,IAAI,CAAC;QACxC,CAAC;QACD,IAAI,WAAW,CAAC,UAAU,CAAC,oBAAoB,CAAC,EAAE,CAAC;YACjD,YAAY,CAAC,iBAAiB,GAAG,IAAI,CAAC;QACxC,CAAC;QACD,IAAI,WAAW,CAAC,UAAU,CAAC,oBAAoB,CAAC,EAAE,CAAC;YACjD,YAAY,CAAC,iBAAiB,GAAG,IAAI,CAAC;QACxC,CAAC;QACD,IAAI,WAAW,CAAC,UAAU,CAAC,YAAY,CAAC,EAAE,CAAC;YACzC,kEAAkE;YAClE,iEAAiE;YACjE,kEAAkE;YAClE,MAAM,cAAc,GAAG,WAAW,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YAC5D,gEAAgE;YAChE,mDAAmD;YACnD,IAAI,UAAU,GAAkB,IAAI,CAAC;YACrC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,cAAc,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBAC/C,MAAM,mBAAmB,GAAG,IAAI,CAAC,IAAI,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;gBACrE,IAAI,UAAU,CAAC,SAAS,CAAC,mBAAmB,CAAC,EAAE,CAAC;oBAC9C,UAAU,GAAG,mBAAmB,CAAC;oBACjC,MAAM;gBACR,CAAC;YACH,CAAC;YACD,IAAI,UAAU,EAAE,CAAC;gBACf,wCAAwC;gBACxC,YAAY,CAAC,SAAS,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;YACzC,CAAC;QACH,CAAC;QACD,IAAI,WAAW,CAAC,UAAU,CAAC,kBAAkB,CAAC,EAAE,CAAC;YAC/C,kEAAkE;YAClE,0BAA0B;YAE1B,MAAM,cAAc,GAAG,WAAW,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;YAE5D,MAAM,8BAA8B,GAAG,cAAc,CAAC,OAAO,CAAC,2BAA2B,CAAC,CAAC;YAC3F,MAAM,gBAAgB,GAAG,cAAc,CAAC,OAAO,CAAC,aAAa,CAAC,CAAC;YAC/D,MAAM,0BAA0B,GAAG,cAAc,CAAC,OAAO,CAAC,uBAAuB,CAAC,CAAC;YAEnF,IAAI,8BAA8B,IAAI,CAAC,EAAE,CAAC;gBACxC,gEAAgE;gBAChE,+BAA+B;gBAC/B,MAAM,gBAAgB,GAAG,IAAI,CAAC,IAAI,CAChC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC,EAAE,8BAA8B,CAAC,CAC3D,CAAC;gBACF,IAAI,UAAU,CAAC,eAAe,CAAC,gBAAgB,CAAC,EAAE,CAAC;oBACjD,IAAI,CAAC,YAAY,CAAC,eAAe,CAAC,gBAAgB,CAAC,EAAE,CAAC;wBACpD,YAAY,CAAC,eAAe,CAAC,gBAAgB,CAAC,GAAG;4BAC/C,WAAW,EAAE,IAAI,GAAG,EAAE;4BACtB,yBAAyB,EAAE,IAAI;yBAChC,CAAC;oBACJ,CAAC;oBACD,YAAY,CAAC,eAAe,CAAC,gBAAgB,CAAC,CAAC,yBAAyB,GAAG,IAAI,CAAC;oBAChF,OAAO;gBACT,CAAC;YACH,CAAC;YAED,0EAA0E;YAC1E,2EAA2E;YAC3E,2BAA2B;YAC3B,IACE,gBAAgB,IAAI,CAAC;gBACrB,0BAA0B,IAAI,CAAC;gBAC/B,0BAA0B,GAAG,gBAAgB,EAC7C,CAAC;gBACD,4DAA4D;gBAC5D,8DAA8D;gBAC9D,6CAA6C;gBAC7C,MAAM,gBAAgB,GAAG,IAAI,CAAC,IAAI,CAAC,GAAG,cAAc,CAAC,KAAK,CAAC,CAAC,EAAE,gBAAgB,CAAC,CAAC,CAAC;gBACjF,MAAM,YAAY,GAAG,IAAI,CAAC,IAAI,CAC5B,GAAG,cAAc,CAAC,KAAK,CAAC,gBAAgB,GAAG,CAAC,EAAE,0BAA0B,CAAC,CAC1E,CAAC;gBAEF,IACE,UAAU,CAAC,eAAe,CAAC,gBAAgB,CAAC;oBAC5C,UAAU,CAAC,eAAe,CAAC,gBAAgB,CAAC,CAAC,WAAW,CAAC,YAAY,CAAC,EACtE,CAAC;oBACD,gDAAgD;oBAChD,6BAA6B;oBAC7B,IAAI,CAAC,YAAY,CAAC,eAAe,CAAC,gBAAgB,CAAC,EAAE,CAAC;wBACpD,YAAY,CAAC,eAAe,CAAC,gBAAgB,CAAC,GAAG;4BAC/C,WAAW,EAAE,IAAI,GAAG,EAAE;4BACtB,yBAAyB,EAAE,KAAK;yBACjC,CAAC;oBACJ,CAAC;oBACD,YAAY,CAAC,eAAe,CAAC,gBAAgB,CAAC,CAAC,WAAW,CAAC,GAAG,CAAC,YAAY,CAAC,CAAC;gBAC/E,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,OAAO,YAAY,CAAC;AACtB,CAAC;AAED;;GAEG;AACH,MAAM,CAAC,KAAK,UAAU,qBAAqB,CAAC,QAAgB;IAC1D,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,UAAU,CAAC,GAAG,CAAC,oBAAoB,EAAE;QAC9D,SAAS,EAAE,QAAQ;KACpB,CAAC,CAAC;IACH,OAAO,MAAM,CAAC,IAAI,CAAC;AACrB,CAAC;AAcD;;;GAGG;AACH,MAAM,CAAC,KAAK,UAAU,UAAU,CAAC,EAC/B,UAAU,EACV,QAAQ,EACR,UAAU,EACV,YAAY,GACM;IAClB,MAAM,eAAe,GAAG,MAAM,qBAAqB,CAAC,QAAQ,CAAC,CAAC;IAE9D,0EAA0E;IAC1E,0DAA0D;IAC1D,MAAM,oBAAoB,GAAiB;QACzC,QAAQ,EAAE,KAAK;QACf,iBAAiB,EAAE,KAAK;QACxB,iBAAiB,EAAE,KAAK;QACxB,iBAAiB,EAAE,KAAK;QACxB,eAAe,EAAE,EAAE;QACnB,SAAS,EAAE,IAAI,GAAG,EAAE;KACrB,CAAC;IAEF,eAAe,CAAC,OAAO,CAAC,CAAC,WAAW,EAAE,EAAE;QACtC,QAAQ,WAAW,CAAC,IAAI,EAAE,CAAC;YACzB,KAAK,UAAU,CAAC;YAChB,KAAK,mBAAmB,CAAC;YACzB,KAAK,mBAAmB,CAAC;YACzB,KAAK,mBAAmB;gBACtB,oBAAoB,CAAC,WAAW,CAAC,IAAI,CAAC,GAAG,IAAI,CAAC;gBAC9C,MAAM;YACR,KAAK,UAAU;gBACb,oBAAoB,CAAC,SAAS,CAAC,GAAG,CAAC,WAAW,CAAC,aAAa,CAAC,CAAC;gBAC9D,MAAM;YACR,KAAK,2BAA2B,CAAC,CAAC,CAAC;gBACjC,MAAM,kBAAkB,GAAG,WAAW,CAAC,oBAAoB,CAAC;gBAC5D,IAAI,CAAC,oBAAoB,CAAC,eAAe,CAAC,kBAAkB,CAAC,EAAE,CAAC;oBAC9D,oBAAoB,CAAC,eAAe,CAAC,kBAAkB,CAAC,GAAG;wBACzD,WAAW,EAAE,IAAI,GAAG,EAAE;wBACtB,yBAAyB,EAAE,IAAI;qBAChC,CAAC;gBACJ,CAAC;gBACD,oBAAoB,CAAC,eAAe,CAAC,kBAAkB,CAAC,CAAC,yBAAyB,GAAG,IAAI,CAAC;gBAC1F,MAAM;YACR,CAAC;YACD,KAAK,uBAAuB,CAAC,CAAC,CAAC;gBAC7B,MAAM,kBAAkB,GAAG,WAAW,CAAC,oBAAoB,CAAC;gBAC5D,MAAM,cAAc,GAAG,WAAW,CAAC,eAAe,CAAC;gBACnD,IAAI,CAAC,oBAAoB,CAAC,eAAe,CAAC,kBAAkB,CAAC,EAAE,CAAC;oBAC9D,oBAAoB,CAAC,eAAe,CAAC,kBAAkB,CAAC,GAAG;wBACzD,WAAW,EAAE,IAAI,GAAG,EAAE;wBACtB,yBAAyB,EAAE,KAAK;qBACjC,CAAC;gBACJ,CAAC;gBACD,oBAAoB,CAAC,eAAe,CAAC,kBAAkB,CAAC,CAAC,WAAW,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;gBACzF,MAAM;YACR,CAAC;QACH,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,MAAM,mBAAmB,GAAG,8BAA8B,CAAC,YAAY,EAAE,UAAU,CAAC,CAAC;IAErF,yEAAyE;IACzE,MAAM,aAAa,GAAoB,EAAE,CAAC;IAC1C,MAAM,aAAa,GAAoB,EAAE,CAAC;IAE1C,4DAA4D;IAC5D,KAAK,MAAM,SAAS,IAAI;QACtB,UAAU;QACV,mBAAmB;QACnB,mBAAmB;QACnB,mBAAmB;KACX,EAAE,CAAC;QACX,MAAM,iBAAiB,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC,IAAI,CAAC,IAAI,CAAC,UAAU,EAAE,SAAS,CAAC,CAAC,CAAC;QAChF,IAAI,iBAAiB,IAAI,CAAC,CAAC,oBAAoB,CAAC,SAAS,CAAC,IAAI,mBAAmB,CAAC,SAAS,CAAC,CAAC,EAAE,CAAC;YAC9F,aAAa,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,SAAS,EAAE,CAAC,CAAC;QAC1C,CAAC;aAAM,IAAI,CAAC,iBAAiB,IAAI,oBAAoB,CAAC,SAAS,CAAC,EAAE,CAAC;YACjE,aAAa,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,SAAS,EAAE,CAAC,CAAC;QAC1C,CAAC;IACH,CAAC;IAED,kBAAkB;IAClB,MAAM,CAAC,IAAI,CAAC,UAAU,CAAC,SAAS,CAAC,CAAC,OAAO,CAAC,CAAC,GAAG,EAAE,EAAE;QAChD,IAAI,CAAC,oBAAoB,CAAC,SAAS,CAAC,GAAG,CAAC,GAAG,CAAC,IAAI,mBAAmB,CAAC,SAAS,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,CAAC;YACvF,aAAa,CAAC,IAAI,CAAC;gBACjB,IAAI,EAAE,UAAU;gBAChB,YAAY,EAAE,GAAG;aAClB,CAAC,CAAC;QACL,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,mCAAmC;IACnC,oBAAoB,CAAC,SAAS,CAAC,OAAO,CAAC,CAAC,GAAG,EAAE,EAAE;QAC7C,IAAI,CAAC,UAAU,CAAC,SAAS,CAAC,GAAG,CAAC,EAAE,CAAC;YAC/B,aAAa,CAAC,IAAI,CAAC;gBACjB,IAAI,EAAE,UAAU;gBAChB,YAAY,EAAE,GAAG;aAClB,CAAC,CAAC;QACL,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,+CAA+C;IAC/C,MAAM,KAAK,CAAC,IAAI,CACd,MAAM,CAAC,OAAO,CAAC,UAAU,CAAC,eAAe,CAAC,EAC1C,KAAK,EAAE,CAAC,IAAI,EAAE,kBAAkB,CAAC,EAAE,EAAE;QACnC,MAAM,qCAAqC,GAAG,MAAM,EAAE,CAAC,UAAU,CAC/D,IAAI,CAAC,IAAI,CAAC,UAAU,EAAE,iBAAiB,EAAE,IAAI,EAAE,2BAA2B,CAAC,CAC5E,CAAC;QACF,IACE,qCAAqC;YACrC,CAAC,CAAC,oBAAoB,CAAC,eAAe,CAAC,IAAI,CAAC,EAAE,yBAAyB;gBACrE,mBAAmB,CAAC,eAAe,CAAC,IAAI,CAAC,EAAE,yBAAyB,CAAC,EACvE,CAAC;YACD,aAAa,CAAC,IAAI,CAAC;gBACjB,IAAI,EAAE,2BAA2B;gBACjC,kBAAkB,EAAE,IAAI;aACzB,CAAC,CAAC;QACL,CAAC;QAED,MAAM,KAAK,CAAC,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,kBAAkB,CAAC,WAAW,CAAC,EAAE,KAAK,EAAE,GAAG,EAAE,EAAE;YAC1E,MAAM,iCAAiC,GAAG,MAAM,EAAE,CAAC,UAAU,CAC3D,IAAI,CAAC,IAAI,CACP,UAAU,EACV,iBAAiB,EACjB,IAAI,EACJ,aAAa,EACb,GAAG,EACH,uBAAuB,CACxB,CACF,CAAC;YACF,IACE,iCAAiC;gBACjC,CAAC,CAAC,oBAAoB,CAAC,eAAe,CAAC,IAAI,CAAC,EAAE,WAAW,EAAE,GAAG,CAAC,GAAG,CAAC;oBACjE,mBAAmB,CAAC,eAAe,CAAC,IAAI,CAAC,EAAE,WAAW,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,EACnE,CAAC;gBACD,aAAa,CAAC,IAAI,CAAC;oBACjB,IAAI,EAAE,uBAAuB;oBAC7B,kBAAkB,EAAE,IAAI;oBACxB,cAAc,EAAE,GAAG;iBACpB,CAAC,CAAC;YACL,CAAC;QACH,CAAC,CAAC,CAAC;IACL,CAAC,CACF,CAAC;IAEF,+DAA+D;IAC/D,MAAM,OAAO,CAAC,GAAG,CACf,MAAM,CAAC,OAAO,CAAC,oBAAoB,CAAC,eAAe,CAAC,CAAC,GAAG,CAAC,KAAK,EAAE,CAAC,IAAI,EAAE,kBAAkB,CAAC,EAAE,EAAE;QAC5F,MAAM,oBAAoB,GAAG,CAAC,CAAC,UAAU,CAAC,eAAe,CAAC,IAAI,CAAC,CAAC;QAChE,MAAM,+BAA+B,GAAG,MAAM,EAAE,CAAC,UAAU,CACzD,IAAI,CAAC,IAAI,CAAC,UAAU,EAAE,iBAAiB,EAAE,IAAI,EAAE,2BAA2B,CAAC,CAC5E,CAAC;QACF,IAAI,CAAC,oBAAoB,IAAI,CAAC,+BAA+B,EAAE,CAAC;YAC9D,aAAa,CAAC,IAAI,CAAC;gBACjB,IAAI,EAAE,2BAA2B;gBACjC,kBAAkB,EAAE,IAAI;aACzB,CAAC,CAAC;QACL,CAAC;QAED,MAAM,OAAO,CAAC,GAAG,CACf,CAAC,GAAG,kBAAkB,CAAC,WAAW,CAAC,CAAC,GAAG,CAAC,KAAK,EAAE,GAAG,EAAE,EAAE;YACpD,MAAM,gBAAgB,GAAG,CAAC,CAAC,UAAU,CAAC,eAAe,CAAC,IAAI,CAAC,EAAE,WAAW,CAAC,GAAG,CAAC,CAAC;YAC9E,MAAM,2BAA2B,GAAG,MAAM,EAAE,CAAC,UAAU,CACrD,IAAI,CAAC,IAAI,CACP,UAAU,EACV,iBAAiB,EACjB,IAAI,EACJ,aAAa,EACb,GAAG,EACH,uBAAuB,CACxB,CACF,CAAC;YACF,IAAI,CAAC,oBAAoB,IAAI,CAAC,gBAAgB,IAAI,CAAC,2BAA2B,EAAE,CAAC;gBAC/E,aAAa,CAAC,IAAI,CAAC;oBACjB,IAAI,EAAE,uBAAuB;oBAC7B,kBAAkB,EAAE,IAAI;oBACxB,cAAc,EAAE,GAAG;iBACpB,CAAC,CAAC;YACL,CAAC;QACH,CAAC,CAAC,CACH,CAAC;IACJ,CAAC,CAAC,CACH,CAAC;IAEF,OAAO,EAAE,aAAa,EAAE,aAAa,EAAE,CAAC;AAC1C,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,qBAAqB,CACzC,UAAkB,EAClB,QAAgB,EAChB,gBAAiC;IAEjC,MAAM,eAAe,GAAyC,EAAE,CAAC;IAEjE,4EAA4E;IAC5E,+EAA+E;IAC/E,8EAA8E;IAC9E,mEAAmE;IACnE,MAAM,EAAE,GAAG,IAAI,EAAE,CAAC,kBAAkB,EAAE,CAAC,CAAC;IAExC,MAAM,KAAK,CAAC,SAAS,CAAC,gBAAgB,EAAE,MAAM,CAAC,uBAAuB,EAAE,KAAK,EAAE,KAAK,EAAE,EAAE;QACtF,MAAM,cAAc,GAAG,kBAAkB,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC;QAE7D,iCAAiC;QACjC,MAAM,SAAS,GAAG,MAAM,EAAE,CAAC;QAE3B,8DAA8D;QAC9D,MAAM,OAAO,GAAG,GAAG,CAAC,MAAM,CACxB;YACE,IAAI,EAAE,IAAI;YACV,GAAG,EAAE,cAAc;SACpB,EACD,CAAC,GAAG,CAAC,CACN,CAAC;QAEF,MAAM,WAAW,GAAG,IAAI,iBAAiB,EAAE,CAAC;QAC5C,OAAO,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;QAE1B,MAAM,IAAI,MAAM,CAAC;YACf,MAAM,EAAE,EAAE;YACV,MAAM,EAAE;gBACN,MAAM,EAAE,MAAM,CAAC,cAAc;gBAC7B,GAAG,EAAE,GAAG,SAAS,SAAS;gBAC1B,IAAI,EAAE,WAAW;aAClB;SACF,CAAC,CAAC,IAAI,EAAE,CAAC;QAEV,eAAe,CAAC,IAAI,CAAC,EAAE,GAAG,KAAK,EAAE,IAAI,EAAE,SAAS,EAAE,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,kFAAkF;IAClF,MAAM,KAAK,CAAC,UAAU,CAAC,GAAG,CAAC,aAAa,EAAE;QACxC,SAAS,EAAE,QAAQ;QACnB,yEAAyE;QACzE,qEAAqE;QACrE,MAAM,EAAE,IAAI,CAAC,SAAS,CAAC,eAAe,CAAC;KACxC,CAAC,CAAC;AACL,CAAC;AAED;;;GAGG;AACH,MAAM,CAAC,KAAK,UAAU,YAAY,CAAC,QAAgB,EAAE,cAA+B;IAClF,IAAI,cAAc,CAAC,MAAM,KAAK,CAAC,EAAE,CAAC;QAChC,6DAA6D;QAC7D,OAAO;IACT,CAAC;IAED,MAAM,KAAK,CAAC,UAAU,CAAC,GAAG,CAAC,aAAa,EAAE;QACxC,SAAS,EAAE,QAAQ;QACnB,yEAAyE;QACzE,qEAAqE;QACrE,MAAM,EAAE,IAAI,CAAC,SAAS,CAAC,cAAc,CAAC;KACvC,CAAC,CAAC;AACL,CAAC;AAED;;;;;;;;;;;;;;;;;;GAkBG;AACH,MAAM,UAAU,+BAA+B,CAAC,QAAgB;IAC9D,MAAM,aAAa,GAAG,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,uBAAuB,EAAE,UAAU,QAAQ,EAAE,CAAC,CAAC;IACtF,OAAO;QACL,IAAI,EAAE,aAAa;QACnB,MAAM,EAAE,aAAa;QACrB,SAAS,EAAE,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,UAAU,EAAE,WAAW,CAAC;QAC5D,MAAM,EAAE,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,UAAU,EAAE,QAAQ,CAAC;QACtD,QAAQ,EAAE,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,UAAU,EAAE,UAAU,CAAC;KAC3D,CAAC;AACJ,CAAC;AASD;;;;;;;;;;;GAWG;AACH,MAAM,UAAU,4BAA4B,CAAC,MAAkC;IAC7E,IAAI,MAAM,CAAC,cAAc,EAAE,CAAC;QAC1B,OAAO,+BAA+B,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC,MAAM,CAAC;IAC3D,CAAC;SAAM,CAAC;QACN,OAAO,MAAM,CAAC,IAAI,CAAC;IACrB,CAAC;AACH,CAAC;AAUD,MAAM,CAAC,KAAK,UAAU,qBAAqB,CAAC,EAC1C,UAAU,EACV,QAAQ,EACR,UAAU,EACV,OAAO,EACP,OAAO,GACsB;IAC7B,IAAI,YAAY,GAAa,EAAE,CAAC;IAChC,IAAI,OAAO,IAAI,OAAO,EAAE,CAAC;QACvB,YAAY,GAAG,MAAM,oBAAoB,CAAC,UAAU,EAAE,OAAO,EAAE,OAAO,CAAC,CAAC;IAC1E,CAAC;IAED,MAAM,EAAE,aAAa,EAAE,aAAa,EAAE,GAAG,MAAM,UAAU,CAAC;QACxD,UAAU;QACV,QAAQ;QACR,UAAU;QACV,YAAY;KACb,CAAC,CAAC;IAEH,MAAM,qBAAqB,CAAC,UAAU,EAAE,QAAQ,EAAE,aAAa,CAAC,CAAC;IACjE,MAAM,YAAY,CAAC,QAAQ,EAAE,aAAa,CAAC,CAAC;IAE5C,OAAO,EAAE,aAAa,EAAE,aAAa,EAAE,CAAC;AAC1C,CAAC;AAED;;GAEG;AACH,MAAM,CAAC,KAAK,UAAU,8BAA8B,CAAC,UAAoB,EAAE,aAAqB;IAC9F,MAAM,SAAS,GAAG,MAAM,eAAe,CAAC;QACtC,MAAM,EAAE,aAAa;QACrB,WAAW,EAAE,aAAa;QAC1B,IAAI,EAAE,qBAAqB;QAC3B,WAAW,EAAE,2CAA2C;KACzD,CAAC,CAAC;IAEH,SAAS,CAAC,mBAAmB,CAAC,KAAK,EAAE,GAAG,EAAE,EAAE;QAC1C,KAAK,MAAM,CAAC,CAAC,EAAE,QAAQ,CAAC,IAAI,UAAU,CAAC,OAAO,EAAE,EAAE,CAAC;YACjD,GAAG,CAAC,IAAI,CAAC,gCAAgC,QAAQ,KAAK,CAAC,GAAG,CAAC,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC;YACrF,MAAM,kCAAkC,CAAC,QAAQ,EAAE,GAAG,CAAC,CAAC;QAC1D,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,OAAO,SAAS,CAAC,aAAa,CAAC;AACjC,CAAC;AAED;;GAEG;AACH,KAAK,UAAU,kCAAkC,CAAC,SAAiB,EAAE,GAAc;IACjF,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,6BAA6B,CAAC,CAAC,CAAC;IACpD,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,gBAAgB,CAAC,GAAG,CAAC,iBAAiB,EAAE,EAAE,SAAS,EAAE,CAAC,CAAC;IAClF,IAAI,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC;IACpC,GAAG,CAAC,OAAO,CAAC,2BAA2B,SAAS,EAAE,CAAC,CAAC;IACpD,SAAS,GAAG,IAAI,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,EAAE,EAAE,SAAS,CAAC,CAAC;IACnD,GAAG,CAAC,OAAO,CAAC,8BAA8B,SAAS,EAAE,CAAC,CAAC;IAEvD,MAAM,QAAQ,GAAG,wBAAwB,CAAC,SAAS,CAAC,CAAC;IACrD,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,kBAAkB,QAAQ,EAAE,CAAC,CAAC,CAAC;IAEnD,MAAM,UAAU,CAAC,UAAU,CAAC,QAAQ,EAAE,EAAE,EAAE,KAAK,IAAI,EAAE;QACnD,GAAG,CAAC,OAAO,CAAC,eAAe,CAAC,CAAC;QAE7B,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,4BAA4B,SAAS,EAAE,CAAC,CAAC,CAAC;QAC9D,MAAM,UAAU,GAAG,MAAM,QAAQ,CAAC,cAAc,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC;QACvE,GAAG,CAAC,OAAO,CAAC,oBAAoB,CAAC,CAAC;QAElC,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,uBAAuB,CAAC,CAAC,CAAC;QAC9C,MAAM,YAAY,GAAG;YACnB,UAAU,EAAE,SAAS;YACrB,QAAQ,EAAE,MAAM,CAAC,SAAS,CAAC;YAC3B,UAAU;SACX,CAAC;QACF,MAAM,YAAY,GAAG,MAAM,qBAAqB,CAAC,YAAY,CAAC,CAAC;QAC/D,oBAAoB,CAAC,YAAY,EAAE,GAAG,CAAC,CAAC;QACxC,GAAG,CAAC,OAAO,CAAC,sBAAsB,CAAC,CAAC;IACtC,CAAC,CAAC,CAAC;IAEH,GAAG,CAAC,OAAO,CAAC,eAAe,CAAC,CAAC;IAE7B,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,KAAK,CAAC,iDAAiD,SAAS,EAAE,CAAC,CAAC,CAAC;AACtF,CAAC;AAED,MAAM,WAAW,GAAG,KAAK,EAAE,QAAgB,EAAE,KAAoB,EAAE,EAAE;IACnE,MAAM,gBAAgB,GAAG,+BAA+B,CAAC,QAAQ,CAAC,CAAC;IACnE,MAAM,YAAY,GAAG,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC,SAAS,EAAE,GAAG,KAAK,CAAC,IAAI,SAAS,CAAC,CAAC;IACnF,MAAM,SAAS,GAAG,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC,MAAM,EAAE,GAAG,KAAK,CAAC,IAAI,SAAS,CAAC,CAAC;IAC7E,MAAM,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC,QAAQ,EAAE,KAAK,CAAC,IAAI,CAAC,CAAC;IACpE,IAAI,kBAAkB,CAAC;IACvB,QAAQ,KAAK,CAAC,IAAI,EAAE,CAAC;QACnB,KAAK,UAAU,CAAC;QAChB,KAAK,mBAAmB,CAAC;QACzB,KAAK,mBAAmB,CAAC;QACzB,KAAK,mBAAmB;YACtB,kBAAkB,GAAG,KAAK,CAAC,IAAI,CAAC;YAChC,MAAM;QACR,KAAK,2BAA2B;YAC9B,IAAI,CAAC,KAAK,CAAC,oBAAoB,EAAE,CAAC;gBAChC,MAAM,IAAI,KAAK,CAAC,6CAA6C,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YAC7E,CAAC;YACD,kBAAkB,GAAG,IAAI,CAAC,IAAI,CAC5B,iBAAiB,EACjB,KAAK,CAAC,oBAAoB,EAC1B,2BAA2B,CAC5B,CAAC;YACF,MAAM;QACR,KAAK,uBAAuB;YAC1B,IAAI,CAAC,KAAK,CAAC,oBAAoB,EAAE,CAAC;gBAChC,MAAM,IAAI,KAAK,CAAC,6CAA6C,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YAC7E,CAAC;YACD,IAAI,CAAC,KAAK,CAAC,eAAe,EAAE,CAAC;gBAC3B,MAAM,IAAI,KAAK,CAAC,wCAAwC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YACxE,CAAC;YACD,kBAAkB,GAAG,IAAI,CAAC,IAAI,CAC5B,iBAAiB,EACjB,KAAK,CAAC,oBAAoB,EAC1B,aAAa,EACb,KAAK,CAAC,eAAe,EACrB,uBAAuB,CACxB,CAAC;YACF,MAAM;QACR,KAAK,UAAU;YACb,IAAI,CAAC,KAAK,CAAC,aAAa,EAAE,CAAC;gBACzB,MAAM,IAAI,KAAK,CAAC,sCAAsC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC;YACtE,CAAC;YACD,kBAAkB,GAAG,IAAI,CAAC,IAAI,CAAC,WAAW,EAAE,KAAK,CAAC,aAAa,CAAC,CAAC;YACjE,MAAM;QACR;YACE,MAAM,IAAI,KAAK,CAAC,0BAA0B,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC;IACvE,CAAC;IACD,MAAM,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC,MAAM,EAAE,kBAAkB,CAAC,CAAC;IAC1E,MAAM,kBAAkB,GAAG,IAAI,CAAC,QAAQ,CAAC,IAAI,CAAC,OAAO,CAAC,UAAU,CAAC,EAAE,UAAU,CAAC,CAAC;IAE/E,kFAAkF;IAClF,IAAI,WAAW,GAAG,KAAK,CAAC;IACxB,IAAI,CAAC;QACH,MAAM,UAAU,GAAG,MAAM,EAAE,CAAC,QAAQ,CAAC,UAAU,CAAC,CAAC;QACjD,IAAI,UAAU,KAAK,kBAAkB,EAAE,CAAC;YACtC,WAAW,GAAG,IAAI,CAAC;QACrB,CAAC;IACH,CAAC;IAAC,OAAO,GAAG,EAAE,CAAC;QACb,+EAA+E;QAC/E,iFAAiF;QACjF,+EAA+E;QAC/E,yCAAyC;QACzC,EAAE;QACF,2EAA2E;QAC3E,4EAA4E;QAC5E,iCAAiC;QACjC,EAAE;QACF,6EAA6E;QAC7E,gCAAgC;QAChC,IAAI,GAAG,CAAC,IAAI,KAAK,QAAQ,IAAI,CAAC,MAAM,EAAE,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,WAAW,EAAE,EAAE,CAAC;YACvE,MAAM,EAAE,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;QAC9B,CAAC;aAAM,IAAI,GAAG,CAAC,IAAI,KAAK,QAAQ,EAAE,CAAC;YACjC,6EAA6E;YAC7E,MAAM,GAAG,CAAC;QACZ,CAAC;IACH,CAAC;IACD,IAAI,WAAW,EAAE,CAAC;QAChB,8DAA8D;QAC9D,+BAA+B;QAC/B,OAAO;IACT,CAAC;IAED,wEAAwE;IACxE,sEAAsE;IACtE,MAAM,EAAE,CAAC,SAAS,CAAC,IAAI,CAAC,OAAO,CAAC,YAAY,CAAC,CAAC,CAAC;IAC/C,MAAM,cAAc,CAAC,MAAM,CAAC,cAAc,EAAE,GAAG,KAAK,CAAC,IAAI,SAAS,EAAE,YAAY,CAAC,CAAC;IAClF,MAAM,EAAE,CAAC,IAAI,CAAC,YAAY,EAAE,SAAS,EAAE,EAAE,SAAS,EAAE,IAAI,EAAE,CAAC,CAAC;IAE5D,8DAA8D;IAC9D,4DAA4D;IAC5D,qDAAqD;IACrD,MAAM,EAAE,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;IAC5B,MAAM,EAAE,CAAC,SAAS,CAAC,UAAU,CAAC,CAAC;IAC/B,MAAM,GAAG,CAAC,OAAO,CAAC;QAChB,IAAI,EAAE,SAAS;QACf,GAAG,EAAE,UAAU;KAChB,CAAC,CAAC;IAEH,qEAAqE;IACrE,6EAA6E;IAC7E,0EAA0E;IAC1E,wEAAwE;IACxE,EAAE;IACF,4EAA4E;IAC5E,yEAAyE;IACzE,qDAAqD;IACrD,MAAM,YAAY,GAAG,kBAAkB,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;IACxD,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,YAAY,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;QAC7C,MAAM,UAAU,GAAG,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC,MAAM,EAAE,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACnF,IAAI,CAAC;YACH,MAAM,IAAI,GAAG,MAAM,EAAE,CAAC,KAAK,CAAC,UAAU,CAAC,CAAC;YACxC,IAAI,IAAI,CAAC,cAAc,EAAE,EAAE,CAAC;gBAC1B,MAAM,EAAE,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;YAC9B,CAAC;iBAAM,IAAI,CAAC,IAAI,CAAC,WAAW,EAAE,EAAE,CAAC;gBAC/B,MAAM,IAAI,KAAK,CAAC,GAAG,UAAU,gCAAgC,CAAC,CAAC;YACjE,CAAC;QACH,CAAC;QAAC,OAAO,GAAG,EAAE,CAAC;YACb,IAAI,GAAG,CAAC,IAAI,KAAK,QAAQ;gBAAE,MAAM,GAAG,CAAC;QACvC,CAAC;IACH,CAAC;IAED,iDAAiD;IACjD,mEAAmE;IACnE,OAAO;IACP,6DAA6D;IAC7D,+DAA+D;IAC/D,6DAA6D;IAC7D,4EAA4E;IAC5E,MAAM,OAAO,GAAG,GAAG,UAAU,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC;IAC9C,MAAM,EAAE,CAAC,aAAa,CAAC,kBAAkB,EAAE,OAAO,CAAC,CAAC;IACpD,MAAM,EAAE,CAAC,MAAM,CAAC,OAAO,EAAE,UAAU,CAAC,CAAC;AACvC,CAAC,CAAC;AAEF,MAAM,gBAAgB,GAAG,IAAI,GAAG,EAAyB,CAAC;AAE1D;;;;;;;;;;GAUG;AACH,MAAM,CAAC,KAAK,UAAU,0BAA0B,CAAC,QAAgB,EAAE,MAAuB;IACxF,IAAI,CAAC,MAAM,CAAC,cAAc,EAAE,CAAC;QAC3B,0DAA0D;QAC1D,OAAO;IACT,CAAC;IAED,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,EAAE,CAAC;QAC3B,MAAM,GAAG,CAAC,MAAM,CAAC,CAAC;IACpB,CAAC;IAED,4FAA4F;IAC5F,4EAA4E;IAC5E,MAAM,QAAQ,GAAG,MAAM,KAAK,CAAC,UAAU,CAAC,GAAG,CAAC,0BAA0B,EAAE;QACtE,SAAS,EAAE,QAAQ;QACnB,UAAU,EAAE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC;KACnC,CAAC,CAAC;IAEH,6EAA6E;IAC7E,4EAA4E;IAC5E,4EAA4E;IAC5E,2EAA2E;IAC3E,6CAA6C;IAC7C,EAAE;IACF,iDAAiD;IACjD,MAAM,WAAW,GAAG,QAAQ,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC;IACtE,MAAM,aAAa,GAAG,QAAQ,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC;IAExE,0EAA0E;IAC1E,iBAAiB;IACjB,MAAM,KAAK,CAAC,SAAS,CAAC,WAAW,EAAE,MAAM,CAAC,yBAAyB,EAAE,KAAK,EAAE,KAAK,EAAE,EAAE;QACnF,MAAM,eAAe,GAAG,GAAG,QAAQ,IAAI,KAAK,CAAC,IAAI,EAAE,CAAC;QACpD,MAAM,mBAAmB,GAAG,gBAAgB,CAAC,GAAG,CAAC,eAAe,CAAC,CAAC;QAClE,IAAI,mBAAmB,EAAE,CAAC;YACxB,oEAAoE;YACpE,OAAO,mBAAmB,CAAC;QAC7B,CAAC;QAED,MAAM,YAAY,GAAG,WAAW,CAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;QAClD,gBAAgB,CAAC,GAAG,CAAC,eAAe,EAAE,YAAY,CAAC,CAAC;QACpD,IAAI,CAAC;YACH,MAAM,YAAY,CAAC;QACrB,CAAC;gBAAS,CAAC;YACT,iEAAiE;YACjE,8DAA8D;YAC9D,qEAAqE;YACrE,oEAAoE;YACpE,sCAAsC;YACtC,gBAAgB,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC;QAC3C,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,2EAA2E;IAC3E,4EAA4E;IAC5E,0EAA0E;IAC1E,sEAAsE;IACtE,aAAa;IACb,EAAE;IACF,6EAA6E;IAC7E,4EAA4E;IAC5E,iDAAiD;IACjD,EAAE;IACF,iFAAiF;IACjF,MAAM,gBAAgB,GAAG,+BAA+B,CAAC,QAAQ,CAAC,CAAC;IACnE,MAAM,OAAO,CAAC,GAAG,CACf,aAAa,CAAC,GAAG,CAAC,KAAK,EAAE,KAAK,EAAE,EAAE;QAChC,yEAAyE;QACzE,uBAAuB;QACvB,MAAM,aAAa,GAAG,8BAA8B,CAAC,KAAK,CAAC,CAAC;QAC5D,MAAM,EAAE,CAAC,MAAM,CAAC,kBAAkB,CAAC,gBAAgB,CAAC,MAAM,EAAE,aAAa,CAAC,CAAC,CAAC;IAC9E,CAAC,CAAC,CACH,CAAC;AACJ,CAAC;AAOD;;;;;GAKG;AACH,MAAM,CAAC,KAAK,UAAU,sBAAsB,CAC1C,QAAuC;IAEvC,IAAI,CAAC,QAAQ,CAAC,kBAAkB;QAAE,OAAO,EAAE,CAAC;IAC5C,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,UAAU,CAAC,GAAG,CAAC,4BAA4B,EAAE;QACtE,WAAW,EAAE,QAAQ,CAAC,EAAE;KACzB,CAAC,CAAC;IACH,MAAM,WAAW,GAAG,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;IACjD,OAAO,WAAW,CAAC;AACrB,CAAC;AAED;;GAEG;AACH,MAAM,UAAU,oBAAoB,CAClC,EAAE,aAAa,EAAE,aAAa,EAAc,EAC5C,GAA+B;IAE/B,IAAI,aAAa,CAAC,MAAM,KAAK,CAAC,IAAI,aAAa,CAAC,MAAM,KAAK,CAAC,EAAE,CAAC;QAC7D,GAAG,CAAC,OAAO,CAAC,oBAAoB,CAAC,CAAC;QAClC,OAAO;IACT,CAAC;IAED,MAAM,KAAK,GAAa,EAAE,CAAC;IAE3B,IAAI,aAAa,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;QAC7B,KAAK,CAAC,IAAI,CAAC,2CAA2C,CAAC,CAAC;QACxD,aAAa,CAAC,OAAO,CAAC,CAAC,KAAK,EAAE,EAAE;YAC9B,KAAK,CAAC,IAAI,CAAC,KAAK,YAAY,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC;QACzC,CAAC,CAAC,CAAC;IACL,CAAC;SAAM,CAAC;QACN,KAAK,CAAC,IAAI,CAAC,2BAA2B,CAAC,CAAC;IAC1C,CAAC;IAED,IAAI,aAAa,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;QAC7B,KAAK,CAAC,IAAI,CAAC,yCAAyC,CAAC,CAAC;QACtD,aAAa,CAAC,OAAO,CAAC,CAAC,KAAK,EAAE,EAAE;YAC9B,KAAK,CAAC,IAAI,CAAC,KAAK,YAAY,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC;QACzC,CAAC,CAAC,CAAC;IACL,CAAC;SAAM,CAAC;QACN,KAAK,CAAC,IAAI,CAAC,yBAAyB,CAAC,CAAC;IACxC,CAAC;IAED,GAAG,CAAC,OAAO,CAAC,KAAK,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC;AAChC,CAAC","sourcesContent":["import * as child_process from 'child_process';\nimport * as path from 'path';\nimport { PassThrough as PassThroughStream } from 'stream';\nimport * as util from 'util';\n\nimport { S3 } from '@aws-sdk/client-s3';\nimport { Upload } from '@aws-sdk/lib-storage';\nimport * as async from 'async';\nimport fs from 'fs-extra';\nimport * as tar from 'tar';\nimport { v4 as uuidv4 } from 'uuid';\n\nimport * as namedLocks from '@prairielearn/named-locks';\nimport { contains } from '@prairielearn/path-utils';\nimport * as sqldb from '@prairielearn/postgres';\n\nimport { getLockNameForCoursePath } from '../models/course.js';\nimport * as courseDB from '../sync/course-db.js';\nimport { type CourseData } from '../sync/course-db.js';\n\nimport { downloadFromS3, makeS3ClientConfig } from './aws.js';\nimport { chalk } from './chalk.js';\nimport { config } from './config.js';\nimport { type ServerJob, createServerJob } from './server-jobs.js';\n\nconst sql = sqldb.loadSqlEquiv(import.meta.url);\n\ntype ChunkType =\n  | 'elements'\n  | 'elementExtensions'\n  | 'clientFilesCourse'\n  | 'serverFilesCourse'\n  | 'clientFilesCourseInstance'\n  | 'clientFilesAssessment'\n  | 'question';\n\ninterface ElementsChunkMetadata {\n  type: 'elements';\n}\n\ninterface ElementExtensionsChunkMetadata {\n  type: 'elementExtensions';\n}\n\ninterface ClientFilesCourseChunkMetadata {\n  type: 'clientFilesCourse';\n}\n\ninterface ServerFilesCourseChunkMetadata {\n  type: 'serverFilesCourse';\n}\n\ninterface ClientFilesCourseInstanceChunkMetadata {\n  type: 'clientFilesCourseInstance';\n  courseInstanceName: string;\n}\n\ninterface ClientFilesAssessmentChunkMetadata {\n  type: 'clientFilesAssessment';\n  courseInstanceName: string;\n  assessmentName: string;\n}\n\ninterface QuestionChunkMetadata {\n  type: 'question';\n  questionName: string;\n}\n\n/**\n * {@link ChunkMetadata} objects are used to refer to chunks according to their\n * human-readable names. For instance, a question chunk has a `questionName` property\n * that corresponds to a QID, not a `questionId` property that corresponds to a\n * database identifier.\n *\n * For chunks that are identified by database IDs instead, see {@link Chunk}.\n *\n */\ntype ChunkMetadata =\n  | ElementsChunkMetadata\n  | ElementExtensionsChunkMetadata\n  | ClientFilesCourseChunkMetadata\n  | ServerFilesCourseChunkMetadata\n  | ClientFilesCourseInstanceChunkMetadata\n  | ClientFilesAssessmentChunkMetadata\n  | QuestionChunkMetadata;\n\ninterface ElementsChunk {\n  type: 'elements';\n}\n\ninterface ElementExtensionsChunk {\n  type: 'elementExtensions';\n}\n\ninterface ClientFilesCourseChunk {\n  type: 'clientFilesCourse';\n}\n\ninterface ServerFilesCourseChunk {\n  type: 'serverFilesCourse';\n}\n\ninterface ClientFilesCourseInstanceChunk {\n  type: 'clientFilesCourseInstance';\n  courseInstanceId: string | number;\n}\n\ninterface ClientFilesAssessmentChunk {\n  type: 'clientFilesAssessment';\n  courseInstanceId: string | number;\n  assessmentId: string | number;\n}\n\nexport interface QuestionChunk {\n  type: 'question';\n  questionId: string | number;\n}\n\n/**\n * {@link Chunk} objects are used to identify chunks by the IDs of their\n * corresponding entities. For instance, a question chunk has a `questionId`\n * property that corresponds to `questions.id` in the database.\n *\n * For chunks that are identified by human-readable names instead, see\n * {@link ChunkMetadata}.\n *\n */\nexport type Chunk =\n  | ElementsChunk\n  | ElementExtensionsChunk\n  | ClientFilesCourseChunk\n  | ServerFilesCourseChunk\n  | ClientFilesCourseInstanceChunk\n  | ClientFilesAssessmentChunk\n  | QuestionChunk;\n\n/**\n * {@link DatabaseChunk} objects represent chunks that we've fetched from the\n * database. They're sort of a superset of {@link Chunk} and {@link ChunkMetadata}\n * objects that contain both the IDs and human-readable names of the chunks.\n */\ninterface DatabaseChunk {\n  id: string | number | null;\n  type: ChunkType;\n  uuid: string;\n  course_id: string | number;\n  course_instance_id?: string | number;\n  course_instance_name?: string;\n  assessment_id?: string | number;\n  assessment_name?: string;\n  question_id?: string | number;\n  question_name?: string;\n}\n\ninterface CourseInstanceChunks {\n  clientFilesCourseInstance: boolean;\n  assessments: Set<string>;\n}\n\ninterface CourseChunks {\n  elements: boolean;\n  elementExtensions: boolean;\n  clientFilesCourse: boolean;\n  serverFilesCourse: boolean;\n  questions: Set<string>;\n  courseInstances: Record<string, CourseInstanceChunks>;\n}\n\n/**\n * Constructs a {@link ChunkMetadata} object from the given {@link DatabaseChunk}\n * object.\n */\nexport function chunkMetadataFromDatabaseChunk(chunk: DatabaseChunk): ChunkMetadata {\n  switch (chunk.type) {\n    case 'elements':\n    case 'elementExtensions':\n    case 'clientFilesCourse':\n    case 'serverFilesCourse':\n      return {\n        type: chunk.type,\n      };\n    case 'clientFilesCourseInstance':\n      if (!chunk.course_instance_name) {\n        throw new Error(`course_instance_name is missing for chunk ${chunk.uuid}`);\n      }\n      return {\n        type: chunk.type,\n        courseInstanceName: chunk.course_instance_name,\n      };\n    case 'clientFilesAssessment':\n      if (!chunk.course_instance_name) {\n        throw new Error(`course_instance_name is missing for chunk ${chunk.uuid}`);\n      }\n      if (!chunk.assessment_name) {\n        throw new Error(`assessment_name is missing for chunk ${chunk.uuid}`);\n      }\n      return {\n        type: chunk.type,\n        courseInstanceName: chunk.course_instance_name,\n        assessmentName: chunk.assessment_name,\n      };\n    case 'question':\n      if (!chunk.question_name) {\n        throw new Error(`question_name is missing for chunk ${chunk.uuid}`);\n      }\n      return {\n        type: chunk.type,\n        questionName: chunk.question_name,\n      };\n  }\n}\n\n/**\n * Returns the path for a given chunk relative to the course's root directory.\n */\nexport function pathForChunk(chunkMetadata: ChunkMetadata): string {\n  switch (chunkMetadata.type) {\n    case 'elements':\n    case 'elementExtensions':\n    case 'clientFilesCourse':\n    case 'serverFilesCourse':\n      return chunkMetadata.type;\n    case 'question':\n      return path.join('questions', chunkMetadata.questionName);\n    case 'clientFilesCourseInstance':\n      return path.join(\n        'courseInstances',\n        chunkMetadata.courseInstanceName,\n        'clientFilesCourseInstance',\n      );\n    case 'clientFilesAssessment':\n      return path.join(\n        'courseInstances',\n        chunkMetadata.courseInstanceName,\n        'assessments',\n        chunkMetadata.assessmentName,\n        'clientFilesAssessment',\n      );\n  }\n}\n\n/**\n * Returns the absolute path for a course's chunk within the course's runtime\n * directory.\n */\nexport function coursePathForChunk(coursePath: string, chunkMetadata: ChunkMetadata): string {\n  return path.join(coursePath, pathForChunk(chunkMetadata));\n}\n\n/**\n * Identifies the files that changes between two commits in a given course.\n *\n * @param coursePath The course directory to diff\n * @param oldHash The old (previous) hash for the diff\n * @param newHash The new (current) hash for the diff\n * @returns List of changed files\n */\nexport async function identifyChangedFiles(\n  coursePath: string,\n  oldHash: string,\n  newHash: string,\n): Promise<string[]> {\n  // In some specific scenarios, the course directory and the root of the course\n  // repository might be different. For example, the example course is usually\n  // manually cloned in production environments, and then the course is added\n  // with the path set to the absolute path of the repo _plus_ `exampleCourse/`.\n  //\n  // In these cases, we need to make sure that the paths we're returning from\n  // this function are relative to the course directory, not the root of the\n  // repository. To do this, we query git itself for the root of the repository,\n  // construct an absolute path for each file, and then trim off the course path.\n  const { stdout: topLevelStdout } = await util.promisify(child_process.exec)(\n    'git rev-parse --show-toplevel',\n    { cwd: coursePath },\n  );\n  const topLevel = topLevelStdout.trim();\n\n  const { stdout: diffStdout } = await util.promisify(child_process.exec)(\n    `git diff --name-only ${oldHash}..${newHash}`,\n    {\n      cwd: coursePath,\n      // This defaults to 1MB of output, however, we've observed in the past that\n      // courses will go long periods of time without syncing, which in turn will\n      // result in a large number of changed files. The largest diff we've seen\n      // is 1.6MB of text; this new value was chosen to give us plenty of\n      // headroom.\n      maxBuffer: 10 * 1024 * 1024,\n    },\n  );\n  const changedFiles = diffStdout.trim().split('\\n');\n\n  // Construct absolute path to all changed files.\n  const absoluteChangedFiles = changedFiles.map((changedFile) => path.join(topLevel, changedFile));\n\n  // Exclude any changed files that aren't in the course directory.\n  const courseChangedFiles = absoluteChangedFiles.filter((absoluteChangedFile) =>\n    contains(coursePath, absoluteChangedFile),\n  );\n\n  // Convert all absolute paths back into relative paths.\n  return courseChangedFiles.map((absoluteChangedFile) =>\n    path.relative(coursePath, absoluteChangedFile),\n  );\n}\n\n/**\n * Given a list of files that have changed (such as that produced by\n * `git diff --name-only`), returns a data structure describing the chunks\n * that need to be generated.\n *\n * @param changedFiles A list of files that changed in a given sync.\n * @param courseData The \"full course\" that was loaded from disk.\n */\nexport function identifyChunksFromChangedFiles(\n  changedFiles: string[],\n  courseData: CourseData,\n): CourseChunks {\n  const courseChunks: CourseChunks = {\n    elements: false,\n    elementExtensions: false,\n    clientFilesCourse: false,\n    serverFilesCourse: false,\n    courseInstances: {},\n    questions: new Set(),\n  };\n\n  changedFiles.forEach((changedFile) => {\n    if (changedFile.startsWith('elements/')) {\n      courseChunks.elements = true;\n    }\n    if (changedFile.startsWith('elementExtensions/')) {\n      courseChunks.elementExtensions = true;\n    }\n    if (changedFile.startsWith('serverFilesCourse/')) {\n      courseChunks.serverFilesCourse = true;\n    }\n    if (changedFile.startsWith('clientFilesCourse/')) {\n      courseChunks.clientFilesCourse = true;\n    }\n    if (changedFile.startsWith('questions/')) {\n      // Here's where things get interesting. Questions can be nested in\n      // directories, so we need to figure out which of the potentially\n      // deeply-nested directories is the root of a particular question.\n      const pathComponents = changedFile.split(path.sep).slice(1);\n      // Progressively join more and more path components until we get\n      // something that corresponds to an actual question\n      let questionId: string | null = null;\n      for (let i = 1; i < pathComponents.length; i++) {\n        const candidateQuestionId = path.join(...pathComponents.slice(0, i));\n        if (courseData.questions[candidateQuestionId]) {\n          questionId = candidateQuestionId;\n          break;\n        }\n      }\n      if (questionId) {\n        // This chunk corresponds to a question!\n        courseChunks.questions.add(questionId);\n      }\n    }\n    if (changedFile.startsWith('courseInstances/')) {\n      // This could be one of two things: `clientFilesCourseInstance` or\n      // `clientFileAssessment`.\n\n      const pathComponents = changedFile.split(path.sep).slice(1);\n\n      const clientFilesCourseInstanceIndex = pathComponents.indexOf('clientFilesCourseInstance');\n      const assessmentsIndex = pathComponents.indexOf('assessments');\n      const clientFilesAssessmentIndex = pathComponents.indexOf('clientFilesAssessment');\n\n      if (clientFilesCourseInstanceIndex >= 0) {\n        // Let's validate that the preceeding path components correspond\n        // to an actual course instance\n        const courseInstanceId = path.join(\n          ...pathComponents.slice(0, clientFilesCourseInstanceIndex),\n        );\n        if (courseData.courseInstances[courseInstanceId]) {\n          if (!courseChunks.courseInstances[courseInstanceId]) {\n            courseChunks.courseInstances[courseInstanceId] = {\n              assessments: new Set(),\n              clientFilesCourseInstance: true,\n            };\n          }\n          courseChunks.courseInstances[courseInstanceId].clientFilesCourseInstance = true;\n          return;\n        }\n      }\n\n      // Important: fall through to account for weird things like people putting\n      // `clientFilesCourseInstance` directories inside of `clientFileAssessment`\n      // for some strange reason.\n      if (\n        assessmentsIndex >= 0 &&\n        clientFilesAssessmentIndex >= 0 &&\n        clientFilesAssessmentIndex > assessmentsIndex\n      ) {\n        // We probably care about this file - let's validate that by\n        // splitting up the path into chunks that hopefully correspond\n        // to course instance IDs and assessment IDs.\n        const courseInstanceId = path.join(...pathComponents.slice(0, assessmentsIndex));\n        const assessmentId = path.join(\n          ...pathComponents.slice(assessmentsIndex + 1, clientFilesAssessmentIndex),\n        );\n\n        if (\n          courseData.courseInstances[courseInstanceId] &&\n          courseData.courseInstances[courseInstanceId].assessments[assessmentId]\n        ) {\n          // This corresponds to something that we need to\n          // create/update a chunk for!\n          if (!courseChunks.courseInstances[courseInstanceId]) {\n            courseChunks.courseInstances[courseInstanceId] = {\n              assessments: new Set(),\n              clientFilesCourseInstance: false,\n            };\n          }\n          courseChunks.courseInstances[courseInstanceId].assessments.add(assessmentId);\n        }\n      }\n    }\n  });\n\n  return courseChunks;\n}\n\n/**\n * Returns all the chunks the are currently stored for the given course.\n */\nexport async function getAllChunksForCourse(courseId: string) {\n  const result = await sqldb.queryAsync(sql.select_course_chunks, {\n    course_id: courseId,\n  });\n  return result.rows;\n}\n\ninterface DiffChunksOptions {\n  coursePath: string;\n  courseId: string;\n  courseData: CourseData;\n  changedFiles: string[];\n}\n\ninterface ChunksDiff {\n  updatedChunks: ChunkMetadata[];\n  deletedChunks: ChunkMetadata[];\n}\n\n/**\n * Given a course ID, computes a list of all chunks that need to be\n * (re)generated.\n */\nexport async function diffChunks({\n  coursePath,\n  courseId,\n  courseData,\n  changedFiles,\n}: DiffChunksOptions): Promise<ChunksDiff> {\n  const rawCourseChunks = await getAllChunksForCourse(courseId);\n\n  // Build a data structure from the result of getAllChunksForCourse so that\n  // we can efficiently query to see if a given chunk exists\n  const existingCourseChunks: CourseChunks = {\n    elements: false,\n    elementExtensions: false,\n    serverFilesCourse: false,\n    clientFilesCourse: false,\n    courseInstances: {},\n    questions: new Set(),\n  };\n\n  rawCourseChunks.forEach((courseChunk) => {\n    switch (courseChunk.type) {\n      case 'elements':\n      case 'elementExtensions':\n      case 'serverFilesCourse':\n      case 'clientFilesCourse':\n        existingCourseChunks[courseChunk.type] = true;\n        break;\n      case 'question':\n        existingCourseChunks.questions.add(courseChunk.question_name);\n        break;\n      case 'clientFilesCourseInstance': {\n        const courseInstanceName = courseChunk.course_instance_name;\n        if (!existingCourseChunks.courseInstances[courseInstanceName]) {\n          existingCourseChunks.courseInstances[courseInstanceName] = {\n            assessments: new Set(),\n            clientFilesCourseInstance: true,\n          };\n        }\n        existingCourseChunks.courseInstances[courseInstanceName].clientFilesCourseInstance = true;\n        break;\n      }\n      case 'clientFilesAssessment': {\n        const courseInstanceName = courseChunk.course_instance_name;\n        const assessmentName = courseChunk.assessment_name;\n        if (!existingCourseChunks.courseInstances[courseInstanceName]) {\n          existingCourseChunks.courseInstances[courseInstanceName] = {\n            assessments: new Set(),\n            clientFilesCourseInstance: false,\n          };\n        }\n        existingCourseChunks.courseInstances[courseInstanceName].assessments.add(assessmentName);\n        break;\n      }\n    }\n  });\n\n  const changedCourseChunks = identifyChunksFromChangedFiles(changedFiles, courseData);\n\n  // Now, let's compute the set of chunks that we need to update or delete.\n  const updatedChunks: ChunkMetadata[] = [];\n  const deletedChunks: ChunkMetadata[] = [];\n\n  // First: elements, clientFilesCourse, and serverFilesCourse\n  for (const chunkType of [\n    'elements',\n    'elementExtensions',\n    'clientFilesCourse',\n    'serverFilesCourse',\n  ] as const) {\n    const hasChunkDirectory = await fs.pathExists(path.join(coursePath, chunkType));\n    if (hasChunkDirectory && (!existingCourseChunks[chunkType] || changedCourseChunks[chunkType])) {\n      updatedChunks.push({ type: chunkType });\n    } else if (!hasChunkDirectory && existingCourseChunks[chunkType]) {\n      deletedChunks.push({ type: chunkType });\n    }\n  }\n\n  // Next: questions\n  Object.keys(courseData.questions).forEach((qid) => {\n    if (!existingCourseChunks.questions.has(qid) || changedCourseChunks.questions.has(qid)) {\n      updatedChunks.push({\n        type: 'question',\n        questionName: qid,\n      });\n    }\n  });\n\n  // Check for any deleted questions.\n  existingCourseChunks.questions.forEach((qid) => {\n    if (!courseData.questions[qid]) {\n      deletedChunks.push({\n        type: 'question',\n        questionName: qid,\n      });\n    }\n  });\n\n  // Next: course instances and their assessments\n  await async.each(\n    Object.entries(courseData.courseInstances),\n    async ([ciid, courseInstanceInfo]) => {\n      const hasClientFilesCourseInstanceDirectory = await fs.pathExists(\n        path.join(coursePath, 'courseInstances', ciid, 'clientFilesCourseInstance'),\n      );\n      if (\n        hasClientFilesCourseInstanceDirectory &&\n        (!existingCourseChunks.courseInstances[ciid]?.clientFilesCourseInstance ||\n          changedCourseChunks.courseInstances[ciid]?.clientFilesCourseInstance)\n      ) {\n        updatedChunks.push({\n          type: 'clientFilesCourseInstance',\n          courseInstanceName: ciid,\n        });\n      }\n\n      await async.each(Object.keys(courseInstanceInfo.assessments), async (tid) => {\n        const hasClientFilesAssessmentDirectory = await fs.pathExists(\n          path.join(\n            coursePath,\n            'courseInstances',\n            ciid,\n            'assessments',\n            tid,\n            'clientFilesAssessment',\n          ),\n        );\n        if (\n          hasClientFilesAssessmentDirectory &&\n          (!existingCourseChunks.courseInstances[ciid]?.assessments?.has(tid) ||\n            changedCourseChunks.courseInstances[ciid]?.assessments?.has(tid))\n        ) {\n          updatedChunks.push({\n            type: 'clientFilesAssessment',\n            courseInstanceName: ciid,\n            assessmentName: tid,\n          });\n        }\n      });\n    },\n  );\n\n  // Check for any deleted course instances or their assessments.\n  await Promise.all(\n    Object.entries(existingCourseChunks.courseInstances).map(async ([ciid, courseInstanceInfo]) => {\n      const courseInstanceExists = !!courseData.courseInstances[ciid];\n      const clientFilesCourseInstanceExists = await fs.pathExists(\n        path.join(coursePath, 'courseInstances', ciid, 'clientFilesCourseInstance'),\n      );\n      if (!courseInstanceExists || !clientFilesCourseInstanceExists) {\n        deletedChunks.push({\n          type: 'clientFilesCourseInstance',\n          courseInstanceName: ciid,\n        });\n      }\n\n      await Promise.all(\n        [...courseInstanceInfo.assessments].map(async (tid) => {\n          const assessmentExists = !!courseData.courseInstances[ciid]?.assessments[tid];\n          const clientFilesAssessmentExists = await fs.pathExists(\n            path.join(\n              coursePath,\n              'courseInstances',\n              ciid,\n              'assessments',\n              tid,\n              'clientFilesAssessment',\n            ),\n          );\n          if (!courseInstanceExists || !assessmentExists || !clientFilesAssessmentExists) {\n            deletedChunks.push({\n              type: 'clientFilesAssessment',\n              courseInstanceName: ciid,\n              assessmentName: tid,\n            });\n          }\n        }),\n      );\n    }),\n  );\n\n  return { updatedChunks, deletedChunks };\n}\n\nexport async function createAndUploadChunks(\n  coursePath: string,\n  courseId: string,\n  chunksToGenerate: ChunkMetadata[],\n) {\n  const generatedChunks: (ChunkMetadata & { uuid: string })[] = [];\n\n  // Share a single S3 client across all uploads. If we created one client per\n  // upload, we'd face a denial of service if someone changed a sufficient number\n  // of chunks in a single commit because we'd be rapidly hammering the EC2 IMDS\n  // with requests for credentials and would likely get rate limited.\n  const s3 = new S3(makeS3ClientConfig());\n\n  await async.eachLimit(chunksToGenerate, config.chunksMaxParallelUpload, async (chunk) => {\n    const chunkDirectory = coursePathForChunk(coursePath, chunk);\n\n    // Generate a UUId for this chunk\n    const chunkUuid = uuidv4();\n\n    // Let's create a tarball for this chunk and send it off to S3\n    const tarball = tar.create(\n      {\n        gzip: true,\n        cwd: chunkDirectory,\n      },\n      ['.'],\n    );\n\n    const passthrough = new PassThroughStream();\n    tarball.pipe(passthrough);\n\n    await new Upload({\n      client: s3,\n      params: {\n        Bucket: config.chunksS3Bucket,\n        Key: `${chunkUuid}.tar.gz`,\n        Body: passthrough,\n      },\n    }).done();\n\n    generatedChunks.push({ ...chunk, uuid: chunkUuid });\n  });\n\n  // Now that the new chunks have been uploaded, update their status in the database\n  await sqldb.queryAsync(sql.insert_chunks, {\n    course_id: courseId,\n    // Force this to a string; otherwise, our code in `sql-db.js` will try to\n    // convert it into a Postgres `ARRAY[...]` type, which we don't want.\n    chunks: JSON.stringify(generatedChunks),\n  });\n}\n\n/**\n * Deletes the specified chunks from the database. Note that they are not\n * deleted from S3 at this time.\n */\nexport async function deleteChunks(courseId: string, chunksToDelete: ChunkMetadata[]) {\n  if (chunksToDelete.length === 0) {\n    // Avoid a round-trip to the DB if there's nothing to delete.\n    return;\n  }\n\n  await sqldb.queryAsync(sql.delete_chunks, {\n    course_id: courseId,\n    // Force this to a string; otherwise, our code in `sql-db.js` will try to\n    // convert it into a Postgres `ARRAY[...]` type, which we don't want.\n    chunks: JSON.stringify(chunksToDelete),\n  });\n}\n\n/**\n * Returns the paths to the chunks directories for the given course\n * ID. The \"downloads\" directory will hold in-progress chunk\n * downloads, the \"chunks\" directory will hold fully-downloaded chunk\n * zip files, the \"unpacked\" directory will hold unpacked zips, and\n * the \"course\" directory is the reconstructed directory hierarchy\n * that mimics the source repo.\n *\n * IMPORTANT: we previously differentiated between `base` and `course` - that\n * is, `course` was a subdirectory of `base`. However, we've since changed\n * that so that the base directory *is* the course directory, and all\n * chunk-related directories are subdirectories of the course directory. This\n * is crucial for the way that we containerize course code execution, as we\n * need any symlinks to refer to point to something within the course directory.\n * Otherwise, when we mount the course directory into a container, the symlinks\n * won't be resolvable.\n *\n * @param courseId The ID of the course in question\n */\nexport function getChunksDirectoriesForCourseId(courseId: string) {\n  const baseDirectory = path.join(config.chunksConsumerDirectory, `course-${courseId}`);\n  return {\n    base: baseDirectory,\n    course: baseDirectory,\n    downloads: path.join(baseDirectory, '__chunks', 'downloads'),\n    chunks: path.join(baseDirectory, '__chunks', 'chunks'),\n    unpacked: path.join(baseDirectory, '__chunks', 'unpacked'),\n  };\n}\n\ninterface CourseWithRuntimeDirectory {\n  /** The database ID of the course. */\n  id: string;\n  /** The path to the course source (not the chunks) */\n  path: string;\n}\n\n/**\n * Returns the absolute path to the course directory that should be used at\n * runtime for things like serving course files, executing question code, etc.\n * If chunks are enabled, this will be same as the \"course\" directory from\n * `getChunksDirectoriesForCourseId`. Otherwise, this returns the path of the\n * course that was passed in. This abstraction allows calling code to not need\n * to know if chunks are enabled or not.\n *\n * This function is designed to take a course object like one would get from\n * `res.locals.course`. If such an object isn't readily available, you can\n * just construct one with a course ID and course path.\n */\nexport function getRuntimeDirectoryForCourse(course: CourseWithRuntimeDirectory): string {\n  if (config.chunksConsumer) {\n    return getChunksDirectoriesForCourseId(course.id).course;\n  } else {\n    return course.path;\n  }\n}\n\ninterface UpdateChunksForCourseOptions {\n  coursePath: string;\n  courseId: string;\n  courseData: CourseData;\n  oldHash?: string | null;\n  newHash?: string | null;\n}\n\nexport async function updateChunksForCourse({\n  coursePath,\n  courseId,\n  courseData,\n  oldHash,\n  newHash,\n}: UpdateChunksForCourseOptions): Promise<ChunksDiff> {\n  let changedFiles: string[] = [];\n  if (oldHash && newHash) {\n    changedFiles = await identifyChangedFiles(coursePath, oldHash, newHash);\n  }\n\n  const { updatedChunks, deletedChunks } = await diffChunks({\n    coursePath,\n    courseId,\n    courseData,\n    changedFiles,\n  });\n\n  await createAndUploadChunks(coursePath, courseId, updatedChunks);\n  await deleteChunks(courseId, deletedChunks);\n\n  return { updatedChunks, deletedChunks };\n}\n\n/**\n * Generates all chunks for a list of courses.\n */\nexport async function generateAllChunksForCourseList(course_ids: string[], authn_user_id: string) {\n  const serverJob = await createServerJob({\n    userId: authn_user_id,\n    authnUserId: authn_user_id,\n    type: 'generate_all_chunks',\n    description: 'Generate all chunks for a list of courses',\n  });\n\n  serverJob.executeInBackground(async (job) => {\n    for (const [i, courseId] of course_ids.entries()) {\n      job.info(`Generating chunks for course ${courseId} [${i + 1}/${course_ids.length}]`);\n      await _generateAllChunksForCourseWithJob(courseId, job);\n    }\n  });\n\n  return serverJob.jobSequenceId;\n}\n\n/**\n * Helper function to generate all chunks for a single course.\n */\nasync function _generateAllChunksForCourseWithJob(course_id: string, job: ServerJob) {\n  job.info(chalk.bold('Looking up course directory'));\n  const result = await sqldb.queryOneRowAsync(sql.select_course_dir, { course_id });\n  let courseDir = result.rows[0].path;\n  job.verbose(`Found course directory: ${courseDir}`);\n  courseDir = path.resolve(process.cwd(), courseDir);\n  job.verbose(`Resolved course directory: ${courseDir}`);\n\n  const lockName = getLockNameForCoursePath(courseDir);\n  job.info(chalk.bold(`Acquiring lock ${lockName}`));\n\n  await namedLocks.doWithLock(lockName, {}, async () => {\n    job.verbose('Acquired lock');\n\n    job.info(chalk.bold(`Loading course data from ${courseDir}`));\n    const courseData = await courseDB.loadFullCourse(course_id, courseDir);\n    job.verbose('Loaded course data');\n\n    job.info(chalk.bold('Generating all chunks'));\n    const chunkOptions = {\n      coursePath: courseDir,\n      courseId: String(course_id),\n      courseData,\n    };\n    const chunkChanges = await updateChunksForCourse(chunkOptions);\n    logChunkChangesToJob(chunkChanges, job);\n    job.verbose('Generated all chunks');\n  });\n\n  job.verbose('Released lock');\n\n  job.info(chalk.green(`Successfully generated chunks for course ID = ${course_id}`));\n}\n\nconst ensureChunk = async (courseId: string, chunk: DatabaseChunk) => {\n  const courseChunksDirs = getChunksDirectoriesForCourseId(courseId);\n  const downloadPath = path.join(courseChunksDirs.downloads, `${chunk.uuid}.tar.gz`);\n  const chunkPath = path.join(courseChunksDirs.chunks, `${chunk.uuid}.tar.gz`);\n  const unpackPath = path.join(courseChunksDirs.unpacked, chunk.uuid);\n  let relativeTargetPath;\n  switch (chunk.type) {\n    case 'elements':\n    case 'elementExtensions':\n    case 'serverFilesCourse':\n    case 'clientFilesCourse':\n      relativeTargetPath = chunk.type;\n      break;\n    case 'clientFilesCourseInstance':\n      if (!chunk.course_instance_name) {\n        throw new Error(`course_instance_name is missing for chunk ${chunk.uuid}`);\n      }\n      relativeTargetPath = path.join(\n        'courseInstances',\n        chunk.course_instance_name,\n        'clientFilesCourseInstance',\n      );\n      break;\n    case 'clientFilesAssessment':\n      if (!chunk.course_instance_name) {\n        throw new Error(`course_instance_name is missing for chunk ${chunk.uuid}`);\n      }\n      if (!chunk.assessment_name) {\n        throw new Error(`assessment_name is missing for chunk ${chunk.uuid}`);\n      }\n      relativeTargetPath = path.join(\n        'courseInstances',\n        chunk.course_instance_name,\n        'assessments',\n        chunk.assessment_name,\n        'clientFilesAssessment',\n      );\n      break;\n    case 'question':\n      if (!chunk.question_name) {\n        throw new Error(`question_name is missing for chunk ${chunk.uuid}`);\n      }\n      relativeTargetPath = path.join('questions', chunk.question_name);\n      break;\n    default:\n      throw new Error(`unknown type for chunk=${JSON.stringify(chunk)}`);\n  }\n  const targetPath = path.join(courseChunksDirs.course, relativeTargetPath);\n  const relativeUnpackPath = path.relative(path.dirname(targetPath), unpackPath);\n\n  // We have a chunk installed if we have a symlink targetPath -> relativeUnpackPath\n  let chunkExists = false;\n  try {\n    const linkString = await fs.readlink(targetPath);\n    if (linkString === relativeUnpackPath) {\n      chunkExists = true;\n    }\n  } catch (err) {\n    // If we encounter an EINVAL error, chances are that we're trying to `readlink`\n    // on a directory. This can occur if a question is renamed to a parent directory,\n    // e.g. renamed from `foo/bar/baz` to `foo/bar`. In this case, we should remove\n    // the existing target path and continue.\n    //\n    // Our syncing code guarantees that if a question `foo/bar` exists, then no\n    // question with that same prefix will exist, so we can always safely remove\n    // the existing target directory.\n    //\n    // If the target path isn't a directory, who knows what state we're in, so we\n    // allow the error to propagate.\n    if (err.code === 'EINVAL' && (await fs.stat(targetPath)).isDirectory()) {\n      await fs.remove(targetPath);\n    } else if (err.code !== 'ENOENT') {\n      // Allow ENOENT errors to continue, because they mean we don't have the chunk\n      throw err;\n    }\n  }\n  if (chunkExists) {\n    // If we have the correct link then this chunk is unpacked and\n    // installed. We're good to go!\n    return;\n  }\n\n  // Otherwise, we need to download and untar the chunk. We'll download it\n  // to the \"downloads\" path first, then rename it to the \"chunks\" path.\n  await fs.ensureDir(path.dirname(downloadPath));\n  await downloadFromS3(config.chunksS3Bucket, `${chunk.uuid}.tar.gz`, downloadPath);\n  await fs.move(downloadPath, chunkPath, { overwrite: true });\n\n  // Once the chunk has been downloaded, we need to untar it. In\n  // case we had an earlier unpack attempt, we will remove the\n  // existing unpack directory to ensure a clean slate.\n  await fs.remove(unpackPath);\n  await fs.ensureDir(unpackPath);\n  await tar.extract({\n    file: chunkPath,\n    cwd: unpackPath,\n  });\n\n  // Before we configure the symlink, we need to check if there are any\n  // outdated symlinks that need to be removed. Those can occur when a question\n  // is renamed into a directory nested inside of its former directory, e.g.\n  // if `questions/a/b/info.json` is moved to `questions/a/b/c/info.json`.\n  //\n  // We'll handle this by checking if any parent directory of the `targetPath`\n  // exists and is a symlink. If so, we'll remove it. This should always be\n  // safe because we should never have nested symlinks.\n  const pathSegments = relativeTargetPath.split(path.sep);\n  for (let i = 1; i < pathSegments.length; i++) {\n    const parentPath = path.join(courseChunksDirs.course, ...pathSegments.slice(0, i));\n    try {\n      const stat = await fs.lstat(parentPath);\n      if (stat.isSymbolicLink()) {\n        await fs.remove(parentPath);\n      } else if (!stat.isDirectory()) {\n        throw new Error(`${parentPath} exists but is not a directory`);\n      }\n    } catch (err) {\n      if (err.code !== 'ENOENT') throw err;\n    }\n  }\n\n  // Finally, link targetPath -> relativeUnpackPath\n  // Note that ensureSymlink() won't overwrite an existing targetPath\n  // See:\n  //     https://github.com/jprichardson/node-fs-extra/pull/869\n  //     https://github.com/jprichardson/node-fs-extra/issues/786\n  //     https://github.com/jprichardson/node-fs-extra/pull/826\n  // As a work-around, we symlink a temporary name and move it over targetPath\n  const tmpPath = `${targetPath}-${chunk.uuid}`;\n  await fs.ensureSymlink(relativeUnpackPath, tmpPath);\n  await fs.rename(tmpPath, targetPath);\n};\n\nconst pendingChunksMap = new Map<string, Promise<void>>();\n\n/**\n * Ensures that specific chunks for a course are loaded. These chunks will either be pulled\n * from S3 if they do not exist, or the existing on-disk chunks will be used if they are\n * still the latest version.\n *\n * For each requested chunk, if the chunk exists on disk but does not exist in\n * the database, the chunk will be removed from the course's runtime directory.\n *\n * @param courseId The ID of the course to load chunks for.\n * @param chunks One or more chunks to load.\n */\nexport async function ensureChunksForCourseAsync(courseId: string, chunks: Chunk | Chunk[]) {\n  if (!config.chunksConsumer) {\n    // We only need to worry if we are a chunk consumer server\n    return;\n  }\n\n  if (!Array.isArray(chunks)) {\n    chunks = [chunks];\n  }\n\n  // First, query the database to identify the UUID + associated name(s) of each desired chunk\n  // \"Names\" in this case refers to question/course instance/assessment names.\n  const dbChunks = await sqldb.queryAsync(sql.select_metadata_for_chunks, {\n    course_id: courseId,\n    chunks_arr: JSON.stringify(chunks),\n  });\n\n  // The results from the database contain information for chunks that exist in\n  // the database, and also for chunks that do _not_ exist in the database. We\n  // use the latter to remove chunks from disk if they no longer correspond to\n  // a directory in the course. We differentiate between the two based on the\n  // presence of an `id` field in the response.\n  //\n  // See the end of this function for more details.\n  const validChunks = dbChunks.rows.filter((chunk) => chunk.id != null);\n  const missingChunks = dbChunks.rows.filter((chunk) => chunk.id == null);\n\n  // Now, ensure each individual chunk is loaded and untarred to the correct\n  // place on disk.\n  await async.eachLimit(validChunks, config.chunksMaxParallelDownload, async (chunk) => {\n    const pendingChunkKey = `${courseId}-${chunk.uuid}`;\n    const pendingChunkPromise = pendingChunksMap.get(pendingChunkKey);\n    if (pendingChunkPromise) {\n      // If this chunk is already being loaded, reuse the existing promise\n      return pendingChunkPromise;\n    }\n\n    const chunkPromise = ensureChunk(courseId, chunk);\n    pendingChunksMap.set(pendingChunkKey, chunkPromise);\n    try {\n      await chunkPromise;\n    } finally {\n      // Once the promise has settled, remove it from our collection of\n      // pending promises. This helps prevent memory leaks and, more\n      // importantly, ensures we don't cache rejected promises - if loading\n      // a chunk fails for some reason, this will ensure we try to load it\n      // again the next time it's requested.\n      pendingChunksMap.delete(pendingChunkKey);\n    }\n  });\n\n  // We also need to take care to remove any chunks that are no longer valid.\n  // For instance, if a course previously had an `elements` directory but that\n  // directory was removed in a more recent revision, we need to ensure that\n  // the `elements` directory does not exist inside the course's runtime\n  // directory.\n  //\n  // For any chunk that the caller is asking to \"ensure\", we check if it exists\n  // in the results of `select_metadata_for_chunks`. If it does not, we remove\n  // the chunk from the course's runtime directory.\n  //\n  // See https://github.com/PrairieLearn/PrairieLearn/issues/4692 for more details.\n  const courseChunksDirs = getChunksDirectoriesForCourseId(courseId);\n  await Promise.all(\n    missingChunks.map(async (chunk) => {\n      // Blindly remove this chunk from disk - if it doesn't exist, `fs.remove`\n      // will silently no-op.\n      const chunkMetadata = chunkMetadataFromDatabaseChunk(chunk);\n      await fs.remove(coursePathForChunk(courseChunksDirs.course, chunkMetadata));\n    }),\n  );\n}\n\ninterface QuestionWithTemplateDirectory {\n  id: string;\n  template_directory: null | string;\n}\n\n/**\n * Get the list of template question IDs for a given question.\n *\n * @param question A question object.\n * @returns Array of question IDs that are (recursive) templates for the given question (may be an empty array).\n */\nexport async function getTemplateQuestionIds(\n  question: QuestionWithTemplateDirectory,\n): Promise<string[]> {\n  if (!question.template_directory) return [];\n  const result = await sqldb.queryAsync(sql.select_template_question_ids, {\n    question_id: question.id,\n  });\n  const questionIds = result.rows.map((r) => r.id);\n  return questionIds;\n}\n\n/**\n * Logs the changes to chunks for a given job.\n */\nexport function logChunkChangesToJob(\n  { updatedChunks, deletedChunks }: ChunksDiff,\n  job: Pick<ServerJob, 'verbose'>,\n) {\n  if (updatedChunks.length === 0 && deletedChunks.length === 0) {\n    job.verbose('No chunks changed.');\n    return;\n  }\n\n  const lines: string[] = [];\n\n  if (updatedChunks.length > 0) {\n    lines.push('Generated chunks for the following paths:');\n    updatedChunks.forEach((chunk) => {\n      lines.push(`  ${pathForChunk(chunk)}`);\n    });\n  } else {\n    lines.push('No chunks were generated.');\n  }\n\n  if (deletedChunks.length > 0) {\n    lines.push('Deleted chunks for the following paths:');\n    deletedChunks.forEach((chunk) => {\n      lines.push(`  ${pathForChunk(chunk)}`);\n    });\n  } else {\n    lines.push('No chunks were deleted.');\n  }\n\n  job.verbose(lines.join('\\n'));\n}\n"]}